<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Understanding Entropy, Cross-Entropy, and KL Divergence</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" href="https://cdn.simplecss.org/simple.min.css" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/a11y-dark.min.css" integrity="sha512-Vj6gPCk8EZlqnoveEyuGyYaWZ1+jyjMPg8g4shwyyNlRQl6d3L9At02ZHQr5K6s5duZl/+YKMnM3/8pDhoUphg==" crossorigin="anonymous" referrerpolicy="no-referrer" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/languages/go.min.js"></script>
<script src="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.js"></script>
<link
  rel="stylesheet"
  href="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.css"
/>
<link rel="stylesheet" href="/static/css/custom.css" />
<script src="/js/main.js"></script>
<script>hljs.highlightAll();hljs.addPlugin(new CopyButtonPlugin());</script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="preamble" class="status">
<div class="site-header">
  <nav class="site-nav">
    <a href="/about.html">About</a>
    <a href="/notes.html">Notes</a>
    <a href="/references.html">References</a>
  </nav>
</div>
</div>
<div id="content" class="content">
<h1 class="title">Understanding Entropy, Cross-Entropy, and KL Divergence</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org0d39a07">What is Entropy Anyway?</a>
<ul>
<li><a href="#orgfe86162">A Weather Example</a></li>
</ul>
</li>
<li><a href="#org3c6ea28">Why Should You Care?</a></li>
<li><a href="#orge6a5cb8">Cross-Entropy: Measuring Real-World Performance</a></li>
<li><a href="#orgf82f84a">KL Divergence: The Efficiency Gap</a></li>
<li><a href="#orga3afa65">Why Machine Learning Loves Cross-Entropy</a></li>
</ul>
</div>
</div>
<p>
Back in 1948, Claude Shannon published his master's thesis, <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">A Mathematical Theory of Communication</a>, essentially inventing information theory in the process. He introduced a simple diagram showing how communication systems work:
</p>


<div id="org9c08dd9" class="figure">
<p><img src="static/images/schematic-comm-system.png" alt="schematic-comm-system.png" />
</p>
</div>

<p>
Then he defined <b>Entropy</b> — a concept that became the foundation for everything we do with information today.
</p>
<div id="outline-container-org0d39a07" class="outline-2">
<h2 id="org0d39a07">What is Entropy Anyway?</h2>
<div class="outline-text-2" id="text-org0d39a07">
<p>
Information entropy gets explained in many ways, but here's my favorite: <b>"How much randomness is floating around in your data?"</b> It's similar to <a href="https://www.wikiwand.com/en/Boltzmann%27s_entropy_formula">Boltzmann's Entropy</a> from physics, if you're familiar with that.
</p>

<p>
The formula looks like this:
</p>

<p>
\[ H = -K \sum_{i = 1}^{n} p_i \log{p_i} \]
</p>

<p>
where \(p_i\) is the probability of event <i>i</i> happening. (The constant K just lets you choose your units — bits, nats, whatever works for you.)
</p>
</div>
<div id="outline-container-orgfe86162" class="outline-3">
<h3 id="orgfe86162">A Weather Example</h3>
<div class="outline-text-3" id="text-orgfe86162">
<p>
Let me show you what this actually means with some weather data:
</p>

<div class="org-src-container">
<pre><code class="python"><span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np
<span style="font-weight: bold; font-style: italic;">H</span> = <span style="font-weight: bold;">lambda</span> xs: -numpy.<span style="font-weight: bold;">sum</span>(<span style="font-weight: bold;">map</span>(<span style="font-weight: bold;">lambda</span> x: x * numpy.log2(x) <span style="font-weight: bold;">if</span> x != 0 <span style="font-weight: bold;">else</span> 0, xs))
<span style="font-weight: bold;">return</span> <span style="font-weight: bold;">list</span>(<span style="font-weight: bold;">map</span>(H, [[0.25, 0.25, 0.25, 0.25], [0.5, 0.2, 0.2, 0.1], 
                     [0.8, 0.1, 0.05, 0.05], [1, 0, 0, 0]]))
</code></pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">Sunny</th>
<th scope="col" class="org-right">Rainy</th>
<th scope="col" class="org-right">Snowy</th>
<th scope="col" class="org-right">Foggy</th>
<th scope="col" class="org-right">Entropy</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0.25</td>
<td class="org-right">0.25</td>
<td class="org-right">0.25</td>
<td class="org-right">0.25</td>
<td class="org-right">2.0</td>
</tr>

<tr>
<td class="org-right">0.5</td>
<td class="org-right">0.2</td>
<td class="org-right">0.2</td>
<td class="org-right">0.1</td>
<td class="org-right">1.76</td>
</tr>

<tr>
<td class="org-right">0.8</td>
<td class="org-right">0.1</td>
<td class="org-right">0.05</td>
<td class="org-right">0.05</td>
<td class="org-right">1.02</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>

<p>
See the pattern? The more uncertain things are, the higher the entropy. When you know something will happen for sure (like that last row with 100% sunny weather), entropy drops to zero. Makes sense, right?
</p>
</div>
</div>
</div>
<div id="outline-container-org3c6ea28" class="outline-2">
<h2 id="org3c6ea28">Why Should You Care?</h2>
<div class="outline-text-2" id="text-org3c6ea28">
<p>
Notice I used base 2 for the logarithm? When you do that, Shannon called the units <b>bits</b> (<a href="https://www.wikiwand.com/en/John_Tukey">J. W. Tukey</a> apparently came up with that name). So you can think of entropy as answering this question:
</p>

<p>
<i>On average, how many bits do I need to send this information?</i>
</p>

<p>
This insight proved incredibly useful for building efficient communication systems. The basic principle: if something happens frequently, use fewer bits to represent it. If it's rare, you can afford to use more bits.
</p>

<p>
But knowing the theoretical minimum is just the starting point. How do you know if your actual system is any good? That's where cross-entropy comes in.
</p>
</div>
</div>
<div id="outline-container-orge6a5cb8" class="outline-2">
<h2 id="orge6a5cb8">Cross-Entropy: Measuring Real-World Performance</h2>
<div class="outline-text-2" id="text-orge6a5cb8">
<p>
\[ H(p, q) = -K \sum_{i = 1}^{n} p_i \log{q_i} \]
</p>

<p>
Here, \(q_i\) is what your system <i>thinks</i> the probability is, while \(p_i\) is the actual probability in the real world.
</p>

<p>
I think of cross-entropy as answering: "How many bits is your system actually using?" (as opposed to the theoretical minimum).
</p>

<p>
The elegant part? If your system is perfect and \(q_i = p_i\) for all events, then cross-entropy equals regular entropy. Your system is as efficient as theoretically possible.
</p>
</div>
</div>
<div id="outline-container-orgf82f84a" class="outline-2">
<h2 id="orgf82f84a">KL Divergence: The Efficiency Gap</h2>
<div class="outline-text-2" id="text-orgf82f84a">
<p>
The difference between cross-entropy and entropy is called <b>Relative Entropy</b> or <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL Divergence</a>. It tells you exactly how far you are from perfect efficiency:
</p>

<p>
\[ D_{KL}(p || q) = H(p, q) - H(p) \]
</p>

<p>
Higher KL divergence means you're wasting more bits — your system isn't as efficient as it could be.
</p>
</div>
</div>
<div id="outline-container-orga3afa65" class="outline-2">
<h2 id="orga3afa65">Why Machine Learning Loves Cross-Entropy</h2>
<div class="outline-text-2" id="text-orga3afa65">
<p>
Cross-entropy shows up everywhere in machine learning as a loss function, especially for classification problems. <a href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy">Here's a detailed example</a> if you want to explore further.
</p>

<p>
The intuition is straightforward:
</p>

<ul class="org-ul">
<li><b>Model is confident and correct</b>: Cross-entropy is low ✓</li>
<li><b>Model is confident but wrong</b>: Cross-entropy shoots up ✗</li>
<li><b>Model is uncertain about everything</b>: Cross-entropy is somewhere in between ~</li>
</ul>

<p>
It's the perfect way to penalize both overconfident wrong predictions and general uncertainty — exactly what you want when training a classifier.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date"><i>Last Modified: January 24, 2026</i></p>
</div>
</body>
</html>
